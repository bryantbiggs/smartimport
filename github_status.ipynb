{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "from github import Github\n",
    "from random import randint\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "pw_file = 'credentials/mongo_pw.txt'\n",
    "if os.path.exists(pw_file): \n",
    "    with open(pw_file, 'r') as f:\n",
    "        pub_ip, mongo_usr, mongo_usr_pw = f.readline().strip().split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to ec2 mongo client\n",
    "client = MongoClient('{0}:27017'.format(pub_ip))\n",
    "\n",
    "# get reference to  resume_db\n",
    "db = client['github_db']\n",
    "\n",
    "# authenticate user for database\n",
    "db.authenticate(mongo_usr, mongo_usr_pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents in \"git_repos_contributors\" collection: 3074\n",
      "Documents in \"git_repos_contributors_backup\" collection: 3030\n",
      "Documents in \"git_repos_docs\" collection: 146833\n",
      "Documents in \"git_repos_docs_backup\" collection: 146685\n",
      "Documents in \"git_repos_meta\" collection: 3073\n",
      "Documents in \"git_repos_meta_backup\" collection: 3029\n",
      "Documents in \"git_repos_subscribers\" collection: 3074\n",
      "Documents in \"git_repos_subscribers_backup\" collection: 3030\n",
      "Documents in \"git_users_followers\" collection: 1398\n",
      "Documents in \"git_users_followers_backup\" collection: 1364\n",
      "Documents in \"git_users_following\" collection: 1398\n",
      "Documents in \"git_users_following_backup\" collection: 1364\n",
      "Documents in \"git_users_meta\" collection: 1397\n",
      "Documents in \"git_users_meta_backup\" collection: 1363\n"
     ]
    }
   ],
   "source": [
    "for name in sorted(db.collection_names()):\n",
    "    #db.drop_collection(name)\n",
    "    print('Documents in \\\"{0}\\\" collection: {1}'.format(name, db[name].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents in \"git_repos_contributors\" collection: 3074\n",
      "Documents in \"git_repos_contributors_backup\" collection: 3030\n",
      "Documents in \"git_repos_docs\" collection: 146833\n",
      "Documents in \"git_repos_docs_backup\" collection: 146685\n",
      "Documents in \"git_repos_meta\" collection: 3073\n",
      "Documents in \"git_repos_meta_backup\" collection: 3029\n",
      "Documents in \"git_repos_subscribers\" collection: 3074\n",
      "Documents in \"git_repos_subscribers_backup\" collection: 3030\n",
      "Documents in \"git_users_followers\" collection: 1398\n",
      "Documents in \"git_users_followers_backup\" collection: 1364\n",
      "Documents in \"git_users_following\" collection: 1398\n",
      "Documents in \"git_users_following_backup\" collection: 1364\n",
      "Documents in \"git_users_meta\" collection: 1397\n",
      "Documents in \"git_users_meta_backup\" collection: 1363\n"
     ]
    }
   ],
   "source": [
    "for name in sorted(db.collection_names()):\n",
    "    #db.drop_collection(name)\n",
    "    print('Documents in \\\"{0}\\\" collection: {1}'.format(name, db[name].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import github private token\n",
    "with open('credentials/token.txt', 'r') as infile:\n",
    "    token = infile.readline().strip()\n",
    "    user = infile.readline().strip()\n",
    "\n",
    "git_client = Github(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_remaining(_type ='core'):\n",
    "    '''\n",
    "    _type = 'core' or 'search'\n",
    "    Return: tuple of remaining rate limit quantity, and time till reset\n",
    "    '''\n",
    "    \n",
    "    rate_limit = git_client.get_rate_limit()\n",
    "    raw = dict(rate_limit.raw_data)\n",
    "    \n",
    "    remaining = int(raw['resources'][_type]['remaining'])\n",
    "    reset = int(raw['resources'][_type]['reset'])\n",
    "    \n",
    "    return (remaining, reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource rate limit remaining: (5000, 1473881417)\n",
      "Search rate limit remaining: (30, 1473877877)\n"
     ]
    }
   ],
   "source": [
    "print('Resource rate limit remaining: {0}'.format(get_remaining('core')))\n",
    "print('Search rate limit remaining: {0}'.format(get_remaining('search')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129 mb\n"
     ]
    }
   ],
   "source": [
    "size = db.command(\"dbstats\")['dataSize']\n",
    "size = size/1000 #kb\n",
    "size = size/1000 #mb\n",
    "print('{0} {1}'.format(int(size), 'mb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('57d0296b8532183ad7e281cc'),\n",
      " 'doc_name': '283042__rules.py',\n",
      " 'file_contents': 'import os import re import inspect from uliweb.utils.common '\n",
      "                  'import log from uliweb.utils.sorteddict import SortedDict '\n",
      "                  'from uliweb.utils.date import now import copy class '\n",
      "                  'ReservedKeyError(Exception):pass __exposes__ = SortedDict() '\n",
      "                  '__no_need_exposed__ =    __class_methods__ =    '\n",
      "                  '__app_rules__ =    __url_route_rules__ =    __url_names__ '\n",
      "                  '=    static_views =    reserved_keys =  , , , , , ,   def '\n",
      "                  'add_rule(map, url, endpoint=None, **kwargs): from '\n",
      "                  'werkzeug.routing import Rule kwargs   = endpoint try: '\n",
      "                  'map.add(Rule(url, **kwargs)) except ValueError as e: '\n",
      "                  'log.info( % (url, endpoint)) raise def merge_rules(): from '\n",
      "                  'itertools import chain s =    index =    for v in '\n",
      "                  'sorted(__no_need_exposed__ + '\n",
      "                  'list(chain(*__exposes__.values())), key=lambda x:x 4 ): '\n",
      "                  'appname, endpoint, url, kw, timestamp = v if in kw: '\n",
      "                  'url_name = kw.pop() else: url_name = endpoint __url_names__ '\n",
      "                  'url_name  = endpoint methods =  y.upper() for y in '\n",
      "                  'kw.get(,   )  methods.sort() key = url, tuple(methods), '\n",
      "                  'kw.get() i = index.get(key, None) if i is not None: s i  = '\n",
      "                  'appname, endpoint, url, kw else: s.append((appname, '\n",
      "                  'endpoint, url, kw)) index key  = len(s)-1 return s def '\n",
      "                  'clear_rules(): global __exposes__, __no_need_exposed__ '\n",
      "                  '__exposes__ =    __no_need_exposed__ =    def '\n",
      "                  'set_app_rules(rules=None): global __app_rules__ '\n",
      "                  '__app_rules__ =    __app_rules__.update(rules or   ) def '\n",
      "                  'set_urlroute_rules(rules=None): global __url_route_rules__ '\n",
      "                  '__url_route_rules__ =    for k, v in (rules or   '\n",
      "                  ').values(): __url_route_rules__.append((re.compile(k), v)) '\n",
      "                  'def get_endpoint(f): if inspect.ismethod(f): clsname = '\n",
      "                  'f.im_class.__name__ endpoint = .join( '\n",
      "                  'f.im_class.__module__, clsname, f.__name__ ) elif '\n",
      "                  'inspect.isfunction(f): endpoint = .join( f.__module__, '\n",
      "                  'f.__name__ ) else: endpoint = f return endpoint def '\n",
      "                  'get_template_args(appname, f): viewname, clsname = , if '\n",
      "                  'inspect.ismethod(f): if not f.im_self: clsname = '\n",
      "                  'f.im_class.__name__ else: clsname = f.im_self.__name__ '\n",
      "                  'viewname = f.__name__ else: viewname = f.__name__ return  '\n",
      "                  ':appname, :clsname, :viewname  def expose(rule=None, '\n",
      "                  '**kwargs): e = Expose(rule, **kwargs) if e.parse_level == '\n",
      "                  '1: return rule else: return e class Expose(object): def '\n",
      "                  '__init__(self, rule=None, restful=False, replace=False, '\n",
      "                  'template=None, layout=None, **kwargs): self.restful = '\n",
      "                  'restful self.replace = replace self.template = template '\n",
      "                  'self.layout = layout if inspect.isfunction(rule) or '\n",
      "                  'inspect.isclass(rule): self.parse_level = 1 self.rule = '\n",
      "                  'None self.kwargs =    self.parse(rule) else: '\n",
      "                  'self.parse_level = 2 self.rule = rule self.kwargs = kwargs '\n",
      "                  'def _get_app_prefix(self, appname): if appname in '\n",
      "                  '__app_rules__: d = __app_rules__ appname  if not d: return '\n",
      "                  'if isinstance(d, str): return d else: return d.get() def '\n",
      "                  '_get_app_subdomin(self, appname): if appname in '\n",
      "                  '__app_rules__: d = __app_rules__ appname  if not d: return '\n",
      "                  'if isinstance(d, str): return else: return d.get() def '\n",
      "                  '_fix_url(self, appname, rule): app_prefix = '\n",
      "                  'self._get_app_prefix(appname) if rule.startswith() and '\n",
      "                  'app_prefix: url = os.path.normcase(os.path.join(app_prefix, '\n",
      "                  'rule.lstrip())).replace(, ) else: if rule.startswith(): url '\n",
      "                  '= rule 1:  else: url = rule if len(url) > 1: url = '\n",
      "                  'url.rstrip() return url def _fix_route(self, rule): for k, '\n",
      "                  'v in __url_route_rules__: if k.match(rule): return k.sub(v, '\n",
      "                  'rule) return rule def _fix_kwargs(self, appname, v): '\n",
      "                  '_subdomain = self._get_app_subdomin(appname) if _subdomain '\n",
      "                  'is None: _subdomain = self.kwargs.get() if _subdomain is '\n",
      "                  'None: return if _subdomain: v   = _subdomain else: v.pop(, '\n",
      "                  'None) def _get_path(self, f): m = f.__module__.split() s '\n",
      "                  '=    for i in m: if not i.startswith(): s.append(i) appname '\n",
      "                  '= .join(s) return appname, .join(s) def parse(self, f): if '\n",
      "                  'inspect.isfunction(f) or inspect.ismethod(f): func, result '\n",
      "                  '= self.parse_function(f) a = __exposes__.setdefault(func,   '\n",
      "                  ') a.append(result) else: self.parse_class(f) def '\n",
      "                  'parse_class(self, f): appname, path = self._get_path(f) '\n",
      "                  'clsname = f.__name__ if self.rule: prefix = self.rule else: '\n",
      "                  'prefix = + .join( path, clsname ) f.__exposed_url__ = '\n",
      "                  'prefix for name in dir(f): func = getattr(f, name) if '\n",
      "                  '(inspect.ismethod(func) or inspect.isfunction(func)) and '\n",
      "                  'not name.startswith(): if hasattr(func, ) and '\n",
      "                  'func.__exposed__: new_endpoint = .join( f.__module__, '\n",
      "                  'f.__name__, name ) if func.im_func in __exposes__: for v in '\n",
      "                  '__exposes__.pop(func.im_func): if func.__no_rule__: rule = '\n",
      "                  'self._get_url(appname, prefix, func) else: _old = '\n",
      "                  'func.__old_rule__.get(v 2 ) if _old.startswith(): rule = '\n",
      "                  '_old 1:  if not rule.startswith(): raise ValueError() else: '\n",
      "                  'rule = os.path.join(prefix, _old).replace(, ) rule = '\n",
      "                  'self._fix_url(appname, rule) func.__old_rule__   = clsname '\n",
      "                  'x = list(v) x 1  = new_endpoint x 2  = rule '\n",
      "                  'func.func_dict   = x self._fix_kwargs(appname, v 3 ) rule = '\n",
      "                  'self._fix_route(rule) __no_need_exposed__.append((v 0 , '\n",
      "                  'new_endpoint, rule, v 3 , now())) else: v = '\n",
      "                  'copy.deepcopy(func.func_dict.get()) if '\n",
      "                  'func.func_dict.get(): rule = v 2  else: rule = '\n",
      "                  'self._get_url(appname, prefix, func) rule = '\n",
      "                  'self._fix_route(rule) if v and new_endpoint != v 1 : if '\n",
      "                  'self.replace: v 3    = v 3 .get() or v 1  func.func_dict   '\n",
      "                  '=  :func.__name__, :func.__old_rule__  , :appname  else: v '\n",
      "                  '2  = rule v 1  = new_endpoint v 4  = now() func.func_dict   '\n",
      "                  '= v self._fix_kwargs(appname, v 3 ) '\n",
      "                  '__no_need_exposed__.append(v) else: rule = '\n",
      "                  'self._get_url(appname, prefix, func) rule = '\n",
      "                  'self._fix_route(rule) endpoint = .join( f.__module__, '\n",
      "                  'clsname, func.__name__ ) kw =    self._fix_kwargs(appname, '\n",
      "                  'kw) x = appname, endpoint, rule, kw, now() '\n",
      "                  '__no_need_exposed__.append(x) func.func_dict   = True '\n",
      "                  'func.func_dict   = list(x) func.func_dict   =  :rule, '\n",
      "                  ':clsname  func.func_dict   = None func.func_dict   = None '\n",
      "                  'func.func_dict   = False def _get_url(self, appname, '\n",
      "                  'prefix, f): args = inspect.getargspec(f) 0  if args: if '\n",
      "                  'inspect.ismethod(f): args = args 1:  args =   % x for x in '\n",
      "                  'args  if f.__name__ in reserved_keys: raise '\n",
      "                  'ReservedKeyError( % f.__name__) prefix = prefix.rstrip() if '\n",
      "                  'self.restful: rule = self._fix_url(appname, .join( prefix  '\n",
      "                  '+ args :1  +  f.__name__  +args 1: )) else: rule = '\n",
      "                  'self._fix_url(appname, .join( prefix, f.__name__  +args)) '\n",
      "                  'return rule def parse_function(self, f): args = '\n",
      "                  'inspect.getargspec(f) 0  if args: args =   % x for x in '\n",
      "                  'args  if f.__name__ in reserved_keys: raise '\n",
      "                  'ReservedKeyError( % f.__name__) appname, path = '\n",
      "                  'self._get_path(f) if self.rule is None: if self.restful: '\n",
      "                  'rule = + .join( path  + args :1  +  f.__name__  + args 1: ) '\n",
      "                  'else: rule = + .join( path, f.__name__  + args) else: '\n",
      "                  'self.rule = self._fix_route(self.rule) rule = self.rule '\n",
      "                  'fixed_url = rule.startswith() rule = self._fix_url(appname, '\n",
      "                  'rule) clsname = if inspect.ismethod(f): if not f.im_self: '\n",
      "                  'clsname = f.im_class.__name__ else: clsname = '\n",
      "                  'f.im_self.__name__ endpoint = .join( f.im_class.__module__, '\n",
      "                  'clsname, f.__name__ ) elif inspect.isfunction(f): endpoint '\n",
      "                  '= .join( f.__module__, f.__name__ ) else: endpoint = f '\n",
      "                  'f.func_dict   = True f.func_dict   = (self.parse_level == '\n",
      "                  '1) or (self.parse_level == 2 and (self.rule is None)) if '\n",
      "                  'not hasattr(f, ): f.func_dict   =    f.func_dict   rule  = '\n",
      "                  'self.rule f.func_dict     = clsname f.func_dict   = '\n",
      "                  'self.template f.func_dict   = self.layout f.func_dict   = '\n",
      "                  'fixed_url kw = self.kwargs.copy() self._fix_kwargs(appname, '\n",
      "                  'kw) return f, (appname, endpoint, rule, kw, now()) def '\n",
      "                  '__call__(self, f): from uliweb.utils.common import '\n",
      "                  'safe_import if isinstance(f, (str, unicode)): try: _, f = '\n",
      "                  'safe_import(f) except: log.error( % f) raise self.parse(f) '\n",
      "                  'return f',\n",
      " 'file_extension': 'py',\n",
      " 'file_name': 'rules.py',\n",
      " 'repo_fullname': 'limodou/uliweb',\n",
      " 'repo_id': 283042,\n",
      " 'repo_name': 'uliweb',\n",
      " 'repo_owner_login': 'limodou'}\n"
     ]
    }
   ],
   "source": [
    "cursor = db['git_repos_docs_backup'].find({'file_extension':'py'})\n",
    "\n",
    "pprint(cursor[69000])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "col = db['git_repos_docs'].find()\n",
    "for i in col[:5]:\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor = db['git_repos_docs_backup'].find({'file_extension':'py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# coding: utf-8 \"\"\" The approach taken is explained below. I decided to do '\n",
      " 'it simply. Initially I was considering parsing the data into some sort of '\n",
      " 'structure and then generating an appropriate README. I am still considering '\n",
      " 'doing it - but for now this should work. The only issue I see is that it '\n",
      " 'only sorts the entries at the lowest level, and that the order of the '\n",
      " 'top-level contents do not match the order of the actual entries. This could '\n",
      " 'be extended by having nested blocks, sorting them recursively and flattening '\n",
      " 'the end structure into a list of lines. Revision 2 maybe ^.^. \"\"\" def '\n",
      " 'main(): # First, we load the current README into memory as an array of lines '\n",
      " \"with open('README.md', 'r') as read_me_file: read_me = \"\n",
      " 'read_me_file.readlines() # Then we cluster the lines together as blocks # '\n",
      " 'Each block represents a collection of lines that should be sorted # This was '\n",
      " 'done by assuming only links ([...](...)) are meant to be sorted # Clustering '\n",
      " 'is done by indentation blocks = [] last_indent = None for line in read_me: '\n",
      " 's_line = line.lstrip() indent = len(line) - len(s_line) if '\n",
      " \"any([s_line.startswith(s) for s in ['* [', '- [']]): if indent == \"\n",
      " 'last_indent: blocks[-1].append(line) else: blocks.append([line]) last_indent '\n",
      " '= indent else: blocks.append([line]) last_indent = None with '\n",
      " \"open('README.md', 'w+') as sorted_file: # Then all of the blocks are sorted \"\n",
      " \"individually blocks = [''.join(sorted(block, key=lambda s: s.lower())) for \"\n",
      " 'block in blocks] # And the result is written back to README.md '\n",
      " 'sorted_file.write(\\'\\'.join(blocks)) if __name__ == \"__main__\": main()')\n",
      "('# flasky extensions. flasky pygments style based on tango style from '\n",
      " 'pygments.style import Style from pygments.token import Keyword, Name, '\n",
      " 'Comment, String, Error, \\\\ Number, Operator, Generic, Whitespace, '\n",
      " 'Punctuation, Other, Literal class FlaskyStyle(Style): background_color = '\n",
      " '\"#f8f8f8\" default_style = \"\" styles = { # No corresponding class for the '\n",
      " 'following: #Text: \"\", # class: \\'\\' Whitespace: \"underline #f8f8f8\", # '\n",
      " 'class: \\'w\\' Error: \"#a40000 border:#ef2929\", # class: \\'err\\' Other: '\n",
      " '\"#000000\", # class \\'x\\' Comment: \"italic #8f5902\", # class: \\'c\\' '\n",
      " 'Comment.Preproc: \"noitalic\", # class: \\'cp\\' Keyword: \"bold #004461\", # '\n",
      " 'class: \\'k\\' Keyword.Constant: \"bold #004461\", # class: \\'kc\\' '\n",
      " 'Keyword.Declaration: \"bold #004461\", # class: \\'kd\\' Keyword.Namespace: '\n",
      " '\"bold #004461\", # class: \\'kn\\' Keyword.Pseudo: \"bold #004461\", # class: '\n",
      " '\\'kp\\' Keyword.Reserved: \"bold #004461\", # class: \\'kr\\' Keyword.Type: \"bold '\n",
      " '#004461\", # class: \\'kt\\' Operator: \"#582800\", # class: \\'o\\' Operator.Word: '\n",
      " '\"bold #004461\", # class: \\'ow\\' - like keywords Punctuation: \"bold #000000\", '\n",
      " \"# class: 'p' # because special names such as Name.Class, Name.Function, etc. \"\n",
      " '# are not recognized as such later in the parsing, we choose them # to look '\n",
      " 'the same as ordinary variables. Name: \"#000000\", # class: \\'n\\' '\n",
      " 'Name.Attribute: \"#c4a000\", # class: \\'na\\' - to be revised Name.Builtin: '\n",
      " '\"#004461\", # class: \\'nb\\' Name.Builtin.Pseudo: \"#3465a4\", # class: \\'bp\\' '\n",
      " 'Name.Class: \"#000000\", # class: \\'nc\\' - to be revised Name.Constant: '\n",
      " '\"#000000\", # class: \\'no\\' - to be revised Name.Decorator: \"#888\", # class: '\n",
      " '\\'nd\\' - to be revised Name.Entity: \"#ce5c00\", # class: \\'ni\\' '\n",
      " 'Name.Exception: \"bold #cc0000\", # class: \\'ne\\' Name.Function: \"#000000\", # '\n",
      " 'class: \\'nf\\' Name.Property: \"#000000\", # class: \\'py\\' Name.Label: '\n",
      " '\"#f57900\", # class: \\'nl\\' Name.Namespace: \"#000000\", # class: \\'nn\\' - to '\n",
      " 'be revised Name.Other: \"#000000\", # class: \\'nx\\' Name.Tag: \"bold #004461\", '\n",
      " '# class: \\'nt\\' - like a keyword Name.Variable: \"#000000\", # class: \\'nv\\' - '\n",
      " 'to be revised Name.Variable.Class: \"#000000\", # class: \\'vc\\' - to be '\n",
      " 'revised Name.Variable.Global: \"#000000\", # class: \\'vg\\' - to be revised '\n",
      " 'Name.Variable.Instance: \"#000000\", # class: \\'vi\\' - to be revised Number: '\n",
      " '\"#990000\", # class: \\'m\\' Literal: \"#000000\", # class: \\'l\\' Literal.Date: '\n",
      " '\"#000000\", # class: \\'ld\\' String: \"#4e9a06\", # class: \\'s\\' '\n",
      " 'String.Backtick: \"#4e9a06\", # class: \\'sb\\' String.Char: \"#4e9a06\", # class: '\n",
      " '\\'sc\\' String.Doc: \"italic #8f5902\", # class: \\'sd\\' - like a comment '\n",
      " 'String.Double: \"#4e9a06\", # class: \\'s2\\' String.Escape: \"#4e9a06\", # class: '\n",
      " '\\'se\\' String.Heredoc: \"#4e9a06\", # class: \\'sh\\' String.Interpol: '\n",
      " '\"#4e9a06\", # class: \\'si\\' String.Other: \"#4e9a06\", # class: \\'sx\\' '\n",
      " 'String.Regex: \"#4e9a06\", # class: \\'sr\\' String.Single: \"#4e9a06\", # class: '\n",
      " '\\'s1\\' String.Symbol: \"#4e9a06\", # class: \\'ss\\' Generic: \"#000000\", # '\n",
      " 'class: \\'g\\' Generic.Deleted: \"#a40000\", # class: \\'gd\\' Generic.Emph: '\n",
      " '\"italic #000000\", # class: \\'ge\\' Generic.Error: \"#ef2929\", # class: \\'gr\\' '\n",
      " 'Generic.Heading: \"bold #000080\", # class: \\'gh\\' Generic.Inserted: '\n",
      " '\"#00A000\", # class: \\'gi\\' Generic.Output: \"#888\", # class: \\'go\\' '\n",
      " 'Generic.Prompt: \"#745334\", # class: \\'gp\\' Generic.Strong: \"bold #000000\", # '\n",
      " 'class: \\'gs\\' Generic.Subheading: \"bold #800080\", # class: \\'gu\\' '\n",
      " 'Generic.Traceback: \"bold #a40000\", # class: \\'gt\\' }')\n",
      "('# -*- coding: utf-8 -*- # # Requests documentation build configuration file, '\n",
      " 'created by # sphinx-quickstart on Fri Feb 19 00:05:47 2016. # # This file is '\n",
      " 'execfile()d with the current directory set to its # containing dir. # # Note '\n",
      " 'that not all possible configuration values are present in this # '\n",
      " 'autogenerated file. # # All configuration values have a default; values that '\n",
      " 'are commented out # serve to show the default. import sys import os # If '\n",
      " 'extensions (or modules to document with autodoc) are in another directory, # '\n",
      " 'add these directories to sys.path here. If the directory is relative to the '\n",
      " '# documentation root, use os.path.abspath to make it absolute, like shown '\n",
      " \"here. #sys.path.insert(0, os.path.abspath('.')) # Insert Requests' path into \"\n",
      " \"the system. sys.path.insert(0, os.path.abspath('..')) sys.path.insert(0, \"\n",
      " \"os.path.abspath('_themes')) import requests from requests import __version__ \"\n",
      " '# -- General configuration ------------------------------------------------ '\n",
      " '# If your documentation needs a minimal Sphinx version, state it here. '\n",
      " \"#needs_sphinx = '1.0' # Add any Sphinx extension module names here, as \"\n",
      " \"strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') \"\n",
      " \"or your custom # ones. extensions = [ 'sphinx.ext.autodoc', \"\n",
      " \"'sphinx.ext.intersphinx', 'sphinx.ext.todo', 'sphinx.ext.viewcode', ] # Add \"\n",
      " 'any paths that contain templates here, relative to this directory. '\n",
      " \"templates_path = ['_templates'] # The suffix(es) of source filenames. # You \"\n",
      " \"can specify multiple suffix as a list of string: # source_suffix = ['.rst', \"\n",
      " \"'.md'] source_suffix = '.rst' # The encoding of source files. \"\n",
      " \"#source_encoding = 'utf-8-sig' # The master toctree document. master_doc = \"\n",
      " \"'index' # General information about the project. project = u'Requests' \"\n",
      " \"copyright = u'2016. A <a \"\n",
      " 'href=\"http://kennethreitz.com/pages/open-projects.html\">Kenneth Reitz</a> '\n",
      " \"Project' author = u'Kenneth Reitz' # The version info for the project you're \"\n",
      " 'documenting, acts as replacement for # |version| and |release|, also used in '\n",
      " 'various other places throughout the # built documents. # # The short X.Y '\n",
      " 'version. version = __version__ # The full version, including alpha/beta/rc '\n",
      " 'tags. release = __version__ # The language for content autogenerated by '\n",
      " 'Sphinx. Refer to documentation # for a list of supported languages. # # This '\n",
      " 'is also used if you do content translation via gettext catalogs. # Usually '\n",
      " 'you set \"language\" from the command line for these cases. language = None # '\n",
      " 'There are two options for replacing |today|: either, you set today to some # '\n",
      " \"non-false value, then it is used: #today = '' # Else, today_fmt is used as \"\n",
      " \"the format for a strftime call. #today_fmt = '%B %d, %Y' # List of patterns, \"\n",
      " 'relative to source directory, that match files and # directories to ignore '\n",
      " \"when looking for source files. exclude_patterns = ['_build'] # The reST \"\n",
      " 'default role (used for this markup: `text`) to use for all # documents. '\n",
      " \"#default_role = None # If true, '()' will be appended to :func: etc. \"\n",
      " 'cross-reference text. add_function_parentheses = False # If true, the '\n",
      " 'current module name will be prepended to all description # unit titles (such '\n",
      " 'as .. function::). add_module_names = True # If true, sectionauthor and '\n",
      " 'moduleauthor directives will be shown in the # output. They are ignored by '\n",
      " 'default. #show_authors = False # The name of the Pygments (syntax '\n",
      " 'highlighting) style to use. pygments_style = '\n",
      " \"'flask_theme_support.FlaskyStyle' # A list of ignored prefixes for module \"\n",
      " 'index sorting. #modindex_common_prefix = [] # If true, keep warnings as '\n",
      " '\"system message\" paragraphs in the built documents. #keep_warnings = False # '\n",
      " 'If true, `todo` and `todoList` produce output, else they produce nothing. '\n",
      " 'todo_include_todos = True # -- Options for HTML output '\n",
      " '---------------------------------------------- # The theme to use for HTML '\n",
      " 'and HTML Help pages. See the documentation for # a list of builtin themes. '\n",
      " \"html_theme = 'alabaster' # Theme options are theme-specific and customize \"\n",
      " 'the look and feel of a theme # further. For a list of options available for '\n",
      " 'each theme, see the # documentation. html_theme_options = { '\n",
      " \"'show_powered_by': False, 'github_user': 'kennethreitz', 'github_repo': \"\n",
      " \"'requests', 'github_banner': True, 'show_related': False } # Add any paths \"\n",
      " 'that contain custom themes here, relative to this directory. '\n",
      " '#html_theme_path = [] # The name for this set of Sphinx documents. If None, '\n",
      " 'it defaults to # \"<project> v<release> documentation\". #html_title = None # '\n",
      " 'A shorter title for the navigation bar. Default is the same as html_title. '\n",
      " '#html_short_title = None # The name of an image file (relative to this '\n",
      " 'directory) to place at the top # of the sidebar. #html_logo = None # The '\n",
      " 'name of an image file (within the static path) to use as favicon of the # '\n",
      " 'docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32 # '\n",
      " 'pixels large. #html_favicon = None # Add any paths that contain custom '\n",
      " 'static files (such as style sheets) here, # relative to this directory. They '\n",
      " 'are copied after the builtin static files, # so a file named \"default.css\" '\n",
      " 'will overwrite the builtin \"default.css\". html_static_path = [\\'_static\\'] # '\n",
      " 'Add any extra paths that contain custom files (such as robots.txt or # '\n",
      " '.htaccess) here, relative to this directory. These files are copied # '\n",
      " 'directly to the root of the documentation. #html_extra_path = [] # If not '\n",
      " \"'', a 'Last updated on:' timestamp is inserted at every page bottom, # using \"\n",
      " \"the given strftime format. #html_last_updated_fmt = '%b %d, %Y' # If true, \"\n",
      " 'SmartyPants will be used to convert quotes and dashes to # typographically '\n",
      " 'correct entities. html_use_smartypants = False # Custom sidebar templates, '\n",
      " \"maps document names to template names. html_sidebars = { 'index': \"\n",
      " \"['sidebarintro.html', 'sourcelink.html', 'searchbox.html', 'hacks.html'], \"\n",
      " \"'**': ['sidebarlogo.html', 'localtoc.html', 'relations.html', \"\n",
      " \"'sourcelink.html', 'searchbox.html', 'hacks.html'] } # Additional templates \"\n",
      " 'that should be rendered to pages, maps page names to # template names. '\n",
      " '#html_additional_pages = {} # If false, no module index is generated. '\n",
      " '#html_domain_indices = True # If false, no index is generated. '\n",
      " '#html_use_index = True # If true, the index is split into individual pages '\n",
      " 'for each letter. #html_split_index = False # If true, links to the reST '\n",
      " 'sources are added to the pages. html_show_sourcelink = False # If true, '\n",
      " '\"Created using Sphinx\" is shown in the HTML footer. Default is True. '\n",
      " 'html_show_sphinx = False # If true, \"(C) Copyright ...\" is shown in the HTML '\n",
      " 'footer. Default is True. html_show_copyright = True # If true, an OpenSearch '\n",
      " 'description file will be output, and all pages will # contain a <link> tag '\n",
      " 'referring to it. The value of this option must be the # base URL from which '\n",
      " \"the finished HTML is served. #html_use_opensearch = '' # This is the file \"\n",
      " 'name suffix for HTML files (e.g. \".xhtml\"). #html_file_suffix = None # '\n",
      " 'Language to be used for generating the HTML full-text search index. # Sphinx '\n",
      " \"supports the following languages: # 'da', 'de', 'en', 'es', 'fi', 'fr', \"\n",
      " \"'hu', 'it', 'ja' # 'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr' \"\n",
      " \"#html_search_language = 'en' # A dictionary with options for the search \"\n",
      " \"language support, empty by default. # Now only 'ja' uses this config value \"\n",
      " \"#html_search_options = {'type': 'default'} # The name of a javascript file \"\n",
      " '(relative to the configuration directory) that # implements a search results '\n",
      " 'scorer. If empty, the default will be used. #html_search_scorer = '\n",
      " \"'scorer.js' # Output file base name for HTML help builder. htmlhelp_basename \"\n",
      " \"= 'Requestsdoc' # -- Options for LaTeX output \"\n",
      " '--------------------------------------------- latex_elements = { # The paper '\n",
      " \"size ('letterpaper' or 'a4paper'). #'papersize': 'letterpaper', # The font \"\n",
      " \"size ('10pt', '11pt' or '12pt'). #'pointsize': '10pt', # Additional stuff \"\n",
      " \"for the LaTeX preamble. #'preamble': '', # Latex figure (float) alignment \"\n",
      " \"#'figure_align': 'htbp', } # Grouping the document tree into LaTeX files. \"\n",
      " 'List of tuples # (source start file, target name, title, # author, '\n",
      " 'documentclass [howto, manual, or own class]). latex_documents = [ '\n",
      " \"(master_doc, 'Requests.tex', u'Requests Documentation', u'Kenneth Reitz', \"\n",
      " \"'manual'), ] # The name of an image file (relative to this directory) to \"\n",
      " 'place at the top of # the title page. #latex_logo = None # For \"manual\" '\n",
      " 'documents, if this is true, then toplevel headings are parts, # not '\n",
      " 'chapters. #latex_use_parts = False # If true, show page references after '\n",
      " 'internal links. #latex_show_pagerefs = False # If true, show URL addresses '\n",
      " 'after external links. #latex_show_urls = False # Documents to append as an '\n",
      " 'appendix to all manuals. #latex_appendices = [] # If false, no module index '\n",
      " 'is generated. #latex_domain_indices = True # -- Options for manual page '\n",
      " 'output --------------------------------------- # One entry per manual page. '\n",
      " 'List of tuples # (source start file, name, description, authors, manual '\n",
      " \"section). man_pages = [ (master_doc, 'requests', u'Requests Documentation', \"\n",
      " '[author], 1) ] # If true, show URL addresses after external links. '\n",
      " '#man_show_urls = False # -- Options for Texinfo output '\n",
      " '------------------------------------------- # Grouping the document tree '\n",
      " 'into Texinfo files. List of tuples # (source start file, target name, title, '\n",
      " 'author, # dir menu entry, description, category) texinfo_documents = [ '\n",
      " \"(master_doc, 'Requests', u'Requests Documentation', author, 'Requests', 'One \"\n",
      " \"line description of project.', 'Miscellaneous'), ] # Documents to append as \"\n",
      " 'an appendix to all manuals. #texinfo_appendices = [] # If false, no module '\n",
      " 'index is generated. #texinfo_domain_indices = True # How to display URL '\n",
      " \"addresses: 'footnote', 'no', or 'inline'. #texinfo_show_urls = 'footnote' # \"\n",
      " 'If true, do not generate a @detailmenu in the \"Top\" node\\'s menu. '\n",
      " '#texinfo_no_detailmenu = False # -- Options for Epub output '\n",
      " '---------------------------------------------- # Bibliographic Dublin Core '\n",
      " 'info. epub_title = project epub_author = author epub_publisher = author '\n",
      " 'epub_copyright = copyright # The basename for the epub file. It defaults to '\n",
      " 'the project name. #epub_basename = project # The HTML theme for the epub '\n",
      " 'output. Since the default themes are not # optimized for small screen space, '\n",
      " 'using the same theme for HTML and epub # output is usually not wise. This '\n",
      " \"defaults to 'epub', a theme designed to save # visual space. #epub_theme = \"\n",
      " \"'epub' # The language of the text. It defaults to the language option # or \"\n",
      " \"'en' if the language is not set. #epub_language = '' # The scheme of the \"\n",
      " \"identifier. Typical schemes are ISBN or URL. #epub_scheme = '' # The unique \"\n",
      " 'identifier of the text. This can be a ISBN number # or the project homepage. '\n",
      " \"#epub_identifier = '' # A unique identification for the text. #epub_uid = '' \"\n",
      " '# A tuple containing the cover image and cover page html template filenames. '\n",
      " '#epub_cover = () # A sequence of (type, uri, title) tuples for the guide '\n",
      " 'element of content.opf. #epub_guide = () # HTML files that should be '\n",
      " 'inserted before the pages created by sphinx. # The format is a list of '\n",
      " 'tuples containing the path and title. #epub_pre_files = [] # HTML files that '\n",
      " 'should be inserted after the pages created by sphinx. # The format is a list '\n",
      " 'of tuples containing the path and title. #epub_post_files = [] # A list of '\n",
      " 'files that should not be packed into the epub file. epub_exclude_files = '\n",
      " \"['search.html'] # The depth of the table of contents in toc.ncx. \"\n",
      " '#epub_tocdepth = 3 # Allow duplicate toc entries. #epub_tocdup = True # '\n",
      " \"Choose between 'default' and 'includehidden'. #epub_tocscope = 'default' # \"\n",
      " 'Fix unsupported image types using the Pillow. #epub_fix_images = False # '\n",
      " 'Scale large images. #epub_max_image_width = 0 # How to display URL '\n",
      " \"addresses: 'footnote', 'no', or 'inline'. #epub_show_urls = 'inline' # If \"\n",
      " 'false, no index is generated. #epub_use_index = True intersphinx_mapping = '\n",
      " \"{'urllib3': ('http://urllib3.readthedocs.io/en/latest', None)}\")\n",
      "''\n",
      "('# -*- coding: utf-8 -*- \"\"\" requests.adapters ~~~~~~~~~~~~~~~~~ This module '\n",
      " 'contains the transport adapters that Requests uses to define and maintain '\n",
      " 'connections. \"\"\" import os.path import socket from .models import Response '\n",
      " 'from .packages.urllib3.poolmanager import PoolManager, proxy_from_url from '\n",
      " '.packages.urllib3.response import HTTPResponse from .packages.urllib3.util '\n",
      " 'import Timeout as TimeoutSauce from .packages.urllib3.util.retry import '\n",
      " 'Retry from .compat import urlparse, basestring from .utils import '\n",
      " '(DEFAULT_CA_BUNDLE_PATH, get_encoding_from_headers, '\n",
      " 'prepend_scheme_if_needed, get_auth_from_url, urldefragauth, select_proxy, '\n",
      " 'to_native_string) from .structures import CaseInsensitiveDict from '\n",
      " '.packages.urllib3.exceptions import ClosedPoolError from '\n",
      " '.packages.urllib3.exceptions import ConnectTimeoutError from '\n",
      " '.packages.urllib3.exceptions import HTTPError as _HTTPError from '\n",
      " '.packages.urllib3.exceptions import MaxRetryError from '\n",
      " '.packages.urllib3.exceptions import NewConnectionError from '\n",
      " '.packages.urllib3.exceptions import ProxyError as _ProxyError from '\n",
      " '.packages.urllib3.exceptions import ProtocolError from '\n",
      " '.packages.urllib3.exceptions import ReadTimeoutError from '\n",
      " '.packages.urllib3.exceptions import SSLError as _SSLError from '\n",
      " '.packages.urllib3.exceptions import ResponseError from .cookies import '\n",
      " 'extract_cookies_to_jar from .exceptions import (ConnectionError, '\n",
      " 'ConnectTimeout, ReadTimeout, SSLError, ProxyError, RetryError, '\n",
      " 'InvalidSchema) from .auth import _basic_auth_str try: from '\n",
      " '.packages.urllib3.contrib.socks import SOCKSProxyManager except ImportError: '\n",
      " 'def SOCKSProxyManager(*args, **kwargs): raise InvalidSchema(\"Missing '\n",
      " 'dependencies for SOCKS support.\") DEFAULT_POOLBLOCK = False DEFAULT_POOLSIZE '\n",
      " '= 10 DEFAULT_RETRIES = 0 DEFAULT_POOL_TIMEOUT = None class '\n",
      " 'BaseAdapter(object): \"\"\"The Base Transport Adapter\"\"\" def __init__(self): '\n",
      " 'super(BaseAdapter, self).__init__() def send(self, request, stream=False, '\n",
      " 'timeout=None, verify=True, cert=None, proxies=None): \"\"\"Sends '\n",
      " 'PreparedRequest object. Returns Response object. :param request: The '\n",
      " ':class:`PreparedRequest <PreparedRequest>` being sent. :param stream: '\n",
      " '(optional) Whether to stream the request content. :param timeout: (optional) '\n",
      " 'How long to wait for the server to send data before giving up, as a float, '\n",
      " 'or a :ref:`(connect timeout, read timeout) <timeouts>` tuple. :type timeout: '\n",
      " 'float or tuple :param verify: (optional) Whether to verify SSL certificates. '\n",
      " ':param cert: (optional) Any user-provided SSL certificate to be trusted. '\n",
      " ':param proxies: (optional) The proxies dictionary to apply to the request. '\n",
      " '\"\"\" raise NotImplementedError def close(self): \"\"\"Cleans up adapter specific '\n",
      " 'items.\"\"\" raise NotImplementedError class HTTPAdapter(BaseAdapter): \"\"\"The '\n",
      " 'built-in HTTP Adapter for urllib3. Provides a general-case interface for '\n",
      " 'Requests sessions to contact HTTP and HTTPS urls by implementing the '\n",
      " 'Transport Adapter interface. This class will usually be created by the '\n",
      " ':class:`Session <Session>` class under the covers. :param pool_connections: '\n",
      " 'The number of urllib3 connection pools to cache. :param pool_maxsize: The '\n",
      " 'maximum number of connections to save in the pool. :param max_retries: The '\n",
      " 'maximum number of retries each connection should attempt. Note, this applies '\n",
      " 'only to failed DNS lookups, socket connections and connection timeouts, '\n",
      " 'never to requests where data has made it to the server. By default, Requests '\n",
      " 'does not retry failed connections. If you need granular control over the '\n",
      " \"conditions under which we retry a request, import urllib3's ``Retry`` class \"\n",
      " 'and pass that instead. :param pool_block: Whether the connection pool should '\n",
      " 'block for connections. Usage:: >>> import requests >>> s = '\n",
      " 'requests.Session() >>> a = requests.adapters.HTTPAdapter(max_retries=3) >>> '\n",
      " 's.mount(\\'http://\\', a) \"\"\" __attrs__ = [\\'max_retries\\', \\'config\\', '\n",
      " \"'_pool_connections', '_pool_maxsize', '_pool_block'] def __init__(self, \"\n",
      " 'pool_connections=DEFAULT_POOLSIZE, pool_maxsize=DEFAULT_POOLSIZE, '\n",
      " 'max_retries=DEFAULT_RETRIES, pool_block=DEFAULT_POOLBLOCK): if max_retries '\n",
      " '== DEFAULT_RETRIES: self.max_retries = Retry(0, read=False) else: '\n",
      " 'self.max_retries = Retry.from_int(max_retries) self.config = {} '\n",
      " 'self.proxy_manager = {} super(HTTPAdapter, self).__init__() '\n",
      " 'self._pool_connections = pool_connections self._pool_maxsize = pool_maxsize '\n",
      " 'self._pool_block = pool_block self.init_poolmanager(pool_connections, '\n",
      " 'pool_maxsize, block=pool_block) def __getstate__(self): return dict((attr, '\n",
      " 'getattr(self, attr, None)) for attr in self.__attrs__) def '\n",
      " \"__setstate__(self, state): # Can't handle by adding 'proxy_manager' to \"\n",
      " 'self.__attrs__ because # self.poolmanager uses a lambda function, which '\n",
      " \"isn't pickleable. self.proxy_manager = {} self.config = {} for attr, value \"\n",
      " 'in state.items(): setattr(self, attr, value) '\n",
      " 'self.init_poolmanager(self._pool_connections, self._pool_maxsize, '\n",
      " 'block=self._pool_block) def init_poolmanager(self, connections, maxsize, '\n",
      " 'block=DEFAULT_POOLBLOCK, **pool_kwargs): \"\"\"Initializes a urllib3 '\n",
      " 'PoolManager. This method should not be called from user code, and is only '\n",
      " 'exposed for use when subclassing the :class:`HTTPAdapter '\n",
      " '<requests.adapters.HTTPAdapter>`. :param connections: The number of urllib3 '\n",
      " 'connection pools to cache. :param maxsize: The maximum number of connections '\n",
      " 'to save in the pool. :param block: Block when no free connections are '\n",
      " 'available. :param pool_kwargs: Extra keyword arguments used to initialize '\n",
      " 'the Pool Manager. \"\"\" # save these values for pickling '\n",
      " 'self._pool_connections = connections self._pool_maxsize = maxsize '\n",
      " 'self._pool_block = block self.poolmanager = '\n",
      " 'PoolManager(num_pools=connections, maxsize=maxsize, block=block, '\n",
      " 'strict=True, **pool_kwargs) def proxy_manager_for(self, proxy, '\n",
      " '**proxy_kwargs): \"\"\"Return urllib3 ProxyManager for the given proxy. This '\n",
      " 'method should not be called from user code, and is only exposed for use when '\n",
      " 'subclassing the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`. :param '\n",
      " 'proxy: The proxy to return a urllib3 ProxyManager for. :param proxy_kwargs: '\n",
      " 'Extra keyword arguments used to configure the Proxy Manager. :returns: '\n",
      " 'ProxyManager :rtype: requests.packages.urllib3.ProxyManager \"\"\" if proxy in '\n",
      " 'self.proxy_manager: manager = self.proxy_manager[proxy] elif '\n",
      " \"proxy.lower().startswith('socks'): username, password = \"\n",
      " 'get_auth_from_url(proxy) manager = self.proxy_manager[proxy] = '\n",
      " 'SOCKSProxyManager( proxy, username=username, password=password, '\n",
      " 'num_pools=self._pool_connections, maxsize=self._pool_maxsize, '\n",
      " 'block=self._pool_block, **proxy_kwargs ) else: proxy_headers = '\n",
      " 'self.proxy_headers(proxy) manager = self.proxy_manager[proxy] = '\n",
      " 'proxy_from_url( proxy, proxy_headers=proxy_headers, '\n",
      " 'num_pools=self._pool_connections, maxsize=self._pool_maxsize, '\n",
      " 'block=self._pool_block, **proxy_kwargs) return manager def cert_verify(self, '\n",
      " 'conn, url, verify, cert): \"\"\"Verify a SSL certificate. This method should '\n",
      " 'not be called from user code, and is only exposed for use when subclassing '\n",
      " 'the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`. :param conn: The '\n",
      " 'urllib3 connection object associated with the cert. :param url: The '\n",
      " 'requested URL. :param verify: Whether we should actually verify the '\n",
      " 'certificate. :param cert: The SSL certificate to verify. \"\"\" if '\n",
      " \"url.lower().startswith('https') and verify: cert_loc = None # Allow \"\n",
      " 'self-specified cert location. if verify is not True: cert_loc = verify if '\n",
      " 'not cert_loc: cert_loc = DEFAULT_CA_BUNDLE_PATH if not cert_loc: raise '\n",
      " 'Exception(\"Could not find a suitable SSL CA certificate bundle.\") '\n",
      " \"conn.cert_reqs = 'CERT_REQUIRED' if not os.path.isdir(cert_loc): \"\n",
      " 'conn.ca_certs = cert_loc else: conn.ca_cert_dir = cert_loc else: '\n",
      " \"conn.cert_reqs = 'CERT_NONE' conn.ca_certs = None conn.ca_cert_dir = None if \"\n",
      " 'cert: if not isinstance(cert, basestring): conn.cert_file = cert[0] '\n",
      " 'conn.key_file = cert[1] else: conn.cert_file = cert def build_response(self, '\n",
      " 'req, resp): \"\"\"Builds a :class:`Response <requests.Response>` object from a '\n",
      " 'urllib3 response. This should not be called from user code, and is only '\n",
      " 'exposed for use when subclassing the :class:`HTTPAdapter '\n",
      " '<requests.adapters.HTTPAdapter>` :param req: The :class:`PreparedRequest '\n",
      " '<PreparedRequest>` used to generate the response. :param resp: The urllib3 '\n",
      " 'response object. :rtype: requests.Response \"\"\" response = Response() # '\n",
      " \"Fallback to None if there's no status_code, for whatever reason. \"\n",
      " \"response.status_code = getattr(resp, 'status', None) # Make headers \"\n",
      " 'case-insensitive. response.headers = CaseInsensitiveDict(getattr(resp, '\n",
      " \"'headers', {})) # Set encoding. response.encoding = \"\n",
      " 'get_encoding_from_headers(response.headers) response.raw = resp '\n",
      " 'response.reason = response.raw.reason if isinstance(req.url, bytes): '\n",
      " \"response.url = req.url.decode('utf-8') else: response.url = req.url # Add \"\n",
      " 'new cookies from the server. extract_cookies_to_jar(response.cookies, req, '\n",
      " 'resp) # Give the Response some context. response.request = req '\n",
      " 'response.connection = self return response def get_connection(self, url, '\n",
      " 'proxies=None): \"\"\"Returns a urllib3 connection for the given URL. This '\n",
      " 'should not be called from user code, and is only exposed for use when '\n",
      " 'subclassing the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`. :param '\n",
      " 'url: The URL to connect to. :param proxies: (optional) A Requests-style '\n",
      " 'dictionary of proxies used on this request. :rtype: '\n",
      " 'requests.packages.urllib3.ConnectionPool \"\"\" proxy = select_proxy(url, '\n",
      " \"proxies) if proxy: proxy = prepend_scheme_if_needed(proxy, 'http') \"\n",
      " 'proxy_manager = self.proxy_manager_for(proxy) conn = '\n",
      " 'proxy_manager.connection_from_url(url) else: # Only scheme should be lower '\n",
      " 'case parsed = urlparse(url) url = parsed.geturl() conn = '\n",
      " 'self.poolmanager.connection_from_url(url) return conn def close(self): '\n",
      " '\"\"\"Disposes of any internal state. Currently, this closes the PoolManager '\n",
      " 'and any active ProxyManager, which closes any pooled connections. \"\"\" '\n",
      " 'self.poolmanager.clear() for proxy in self.proxy_manager.values(): '\n",
      " 'proxy.clear() def request_url(self, request, proxies): \"\"\"Obtain the url to '\n",
      " 'use when making the final request. If the message is being sent through a '\n",
      " 'HTTP proxy, the full URL has to be used. Otherwise, we should only use the '\n",
      " 'path portion of the URL. This should not be called from user code, and is '\n",
      " 'only exposed for use when subclassing the :class:`HTTPAdapter '\n",
      " '<requests.adapters.HTTPAdapter>`. :param request: The '\n",
      " ':class:`PreparedRequest <PreparedRequest>` being sent. :param proxies: A '\n",
      " 'dictionary of schemes or schemes and hosts to proxy URLs. :rtype: str \"\"\" '\n",
      " 'proxy = select_proxy(request.url, proxies) scheme = '\n",
      " 'urlparse(request.url).scheme is_proxied_http_request = (proxy and scheme != '\n",
      " \"'https') using_socks_proxy = False if proxy: proxy_scheme = \"\n",
      " 'urlparse(proxy).scheme.lower() using_socks_proxy = '\n",
      " \"proxy_scheme.startswith('socks') url = request.path_url if \"\n",
      " 'is_proxied_http_request and not using_socks_proxy: url = '\n",
      " 'urldefragauth(request.url) return url def add_headers(self, request, '\n",
      " '**kwargs): \"\"\"Add any headers needed by the connection. As of v2.0 this does '\n",
      " 'nothing by default, but is left for overriding by users that subclass the '\n",
      " ':class:`HTTPAdapter <requests.adapters.HTTPAdapter>`. This should not be '\n",
      " 'called from user code, and is only exposed for use when subclassing the '\n",
      " ':class:`HTTPAdapter <requests.adapters.HTTPAdapter>`. :param request: The '\n",
      " ':class:`PreparedRequest <PreparedRequest>` to add headers to. :param kwargs: '\n",
      " 'The keyword arguments from the call to send(). \"\"\" pass def '\n",
      " 'proxy_headers(self, proxy): \"\"\"Returns a dictionary of the headers to add to '\n",
      " 'any request sent through a proxy. This works with urllib3 magic to ensure '\n",
      " 'that they are correctly sent to the proxy, rather than in a tunnelled '\n",
      " 'request if CONNECT is being used. This should not be called from user code, '\n",
      " 'and is only exposed for use when subclassing the :class:`HTTPAdapter '\n",
      " '<requests.adapters.HTTPAdapter>`. :param proxies: The url of the proxy being '\n",
      " 'used for this request. :rtype: dict \"\"\" headers = {} username, password = '\n",
      " 'get_auth_from_url(proxy) if username and password: '\n",
      " \"headers['Proxy-Authorization'] = _basic_auth_str(username, password) return \"\n",
      " 'headers def send(self, request, stream=False, timeout=None, verify=True, '\n",
      " 'cert=None, proxies=None): \"\"\"Sends PreparedRequest object. Returns Response '\n",
      " 'object. :param request: The :class:`PreparedRequest <PreparedRequest>` being '\n",
      " 'sent. :param stream: (optional) Whether to stream the request content. '\n",
      " ':param timeout: (optional) How long to wait for the server to send data '\n",
      " 'before giving up, as a float, or a :ref:`(connect timeout, read timeout) '\n",
      " '<timeouts>` tuple. :type timeout: float or tuple :param verify: (optional) '\n",
      " 'Whether to verify SSL certificates. :param cert: (optional) Any '\n",
      " 'user-provided SSL certificate to be trusted. :param proxies: (optional) The '\n",
      " 'proxies dictionary to apply to the request. :rtype: requests.Response \"\"\" '\n",
      " 'conn = self.get_connection(request.url, proxies) self.cert_verify(conn, '\n",
      " 'request.url, verify, cert) url = self.request_url(request, proxies) '\n",
      " 'self.add_headers(request) chunked = not (request.body is None or '\n",
      " \"'Content-Length' in request.headers) if isinstance(timeout, tuple): try: \"\n",
      " 'connect, read = timeout timeout = TimeoutSauce(connect=connect, read=read) '\n",
      " 'except ValueError as e: # this may raise a string formatting error. err = '\n",
      " '(\"Invalid timeout {0}. Pass a (connect, read) \" \"timeout tuple, or a single '\n",
      " 'float to set \" \"both timeouts to the same value\".format(timeout)) raise '\n",
      " 'ValueError(err) else: timeout = TimeoutSauce(connect=timeout, read=timeout) '\n",
      " 'try: if not chunked: resp = conn.urlopen( method=request.method, url=url, '\n",
      " 'body=request.body, headers=request.headers, redirect=False, '\n",
      " 'assert_same_host=False, preload_content=False, decode_content=False, '\n",
      " 'retries=self.max_retries, timeout=timeout ) # Send the request. else: if '\n",
      " \"hasattr(conn, 'proxy_pool'): conn = conn.proxy_pool low_conn = \"\n",
      " 'conn._get_conn(timeout=DEFAULT_POOL_TIMEOUT) try: '\n",
      " 'low_conn.putrequest(request.method, url, skip_accept_encoding=True) for '\n",
      " 'header, value in request.headers.items(): low_conn.putheader(header, value) '\n",
      " 'low_conn.endheaders() for i in request.body: '\n",
      " \"low_conn.send(hex(len(i))[2:].encode('utf-8')) low_conn.send(b'\\\\r ') \"\n",
      " \"low_conn.send(i) low_conn.send(b'\\\\r ') low_conn.send(b'0\\\\r \\\\r ') # \"\n",
      " 'Receive the response from the server try: # For Python 2.7+ versions, use '\n",
      " 'buffering of HTTP # responses r = low_conn.getresponse(buffering=True) '\n",
      " 'except TypeError: # For compatibility with Python 2.6 versions and back r = '\n",
      " 'low_conn.getresponse() resp = HTTPResponse.from_httplib( r, pool=conn, '\n",
      " 'connection=low_conn, preload_content=False, decode_content=False ) except: # '\n",
      " 'If we hit any problems here, clean up the connection. # Then, reraise so '\n",
      " 'that we can handle the actual exception. low_conn.close() raise except '\n",
      " '(ProtocolError, socket.error) as err: raise ConnectionError(err, '\n",
      " 'request=request) except MaxRetryError as e: if isinstance(e.reason, '\n",
      " 'ConnectTimeoutError): # TODO: Remove this in 3.0.0: see #2811 if not '\n",
      " 'isinstance(e.reason, NewConnectionError): raise ConnectTimeout(e, '\n",
      " 'request=request) if isinstance(e.reason, ResponseError): raise RetryError(e, '\n",
      " 'request=request) if isinstance(e.reason, _ProxyError): raise ProxyError(e, '\n",
      " 'request=request) raise ConnectionError(e, request=request) except '\n",
      " 'ClosedPoolError as e: raise ConnectionError(e, request=request) except '\n",
      " '_ProxyError as e: raise ProxyError(e) except (_SSLError, _HTTPError) as e: '\n",
      " 'if isinstance(e, _SSLError): raise SSLError(e, request=request) elif '\n",
      " 'isinstance(e, ReadTimeoutError): raise ReadTimeout(e, request=request) else: '\n",
      " 'raise return self.build_response(request, resp)')\n"
     ]
    }
   ],
   "source": [
    "for c in cursor[:5]:\n",
    "    pprint(c['file_contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
