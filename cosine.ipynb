{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from six import iteritems\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is this Python project Describe features ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contributing Your contributions are always wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awesome Python Awesome A curated list of aweso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contribution Guidelines Before opening any iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contributor Code of Conduct As contributors an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file_contents\n",
       "0  What is this Python project Describe features ...\n",
       "1  Contributing Your contributions are always wel...\n",
       "2  Awesome Python Awesome A curated list of aweso...\n",
       "3  Contribution Guidelines Before opening any iss...\n",
       "4  Contributor Code of Conduct As contributors an..."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('data/md_contents.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Series to List of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is this Python project Describe features What s the difference between this Python project and similar ones Enumerate comparisons Anyone who agrees with this pull request could vote for it by adding a to it and usually the maintainer will merge it when votes reach']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = df['file_contents'].tolist()\n",
    "contents[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Strings to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the documents, remove stop words and words that only appear once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6283"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set(stopwords.words('english'))\n",
    "\n",
    "texts = [[word.lower() for word in content.split()if word.lower() not in stoplist] for content in contents]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# remove words that occur less than n times\n",
    "texts = [[token for token in text if frequency[token] > 3] for text in texts]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Token Count Dictionary to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(24712 unique tokens: ['connector', 'mattdesl', 'hdf', 'codrops', 'pgdata']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# store the dictionary, for future reference\n",
    "dictionary.save('data/text_token.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Tokenized Resumes to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('data/text_token.mm', corpus)  # store to disk, for later use\n",
    "for c in corpus[:1]:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dictionary LOADED as 'dictionary'\n"
     ]
    }
   ],
   "source": [
    "# load tokenized dictionary\n",
    "if (os.path.exists('data/text_token.dict')):\n",
    "    dictionary = corpora.Dictionary.load('data/text_token.dict')\n",
    "    print('Tokenized dictionary LOADED as \\'dictionary\\'')\n",
    "else:\n",
    "    print('Tokenized dictionary NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix LOADED as 'corpus'\n"
     ]
    }
   ],
   "source": [
    "# load sparse vector matrix\n",
    "if (os.path.exists('data/text_token.mm')):\n",
    "    corpus = corpora.MmCorpus('data/text_token.mm')\n",
    "    print('Sparse matrix LOADED as \\'corpus\\'')\n",
    "else:\n",
    "    print('Sparse matrix NOT FOUND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step 1 -- initialize a model\n",
    "tfidf_mdl = models.TfidfModel(corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `model[corpus]` only creates a wrapper around the old corpus document stream – actual conversions are done on-the-fly, during document iteration. We cannot convert the entire corpus at the time of calling corpus_transformed = model[corpus], because that would mean storing the result in main memory, and that contradicts gensim’s objective of memory-indepedence. If you will be iterating over the transformed corpus_transformed multiple times, and the transformation is costly, serialize the resulting corpus to disk first and continue using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6283\n",
      "[(0, 0.18246551926719032), (1, 0.2270398016185427), (2, 0.19115802241083493), (3, 0.32424049625629037), (4, 0.2890335590555953), (5, 0.3215947393368704), (6, 0.19924725453495884), (7, 0.11147295903409228), (8, 0.39548953276357063), (9, 0.1313466860729142), (10, 0.21634460975497669), (11, 0.27052564249680583), (12, 0.14138878072706235), (13, 0.11714476712623356), (14, 0.1413094453050044), (15, 0.14304380102152006), (16, 0.15635387384423827), (17, 0.20734585403870268), (18, 0.17836280430264478), (19, 0.22853002270124395), (20, 0.12297155708721454)]\n"
     ]
    }
   ],
   "source": [
    "# step 2 -- use the model to transform vectors\n",
    "corpus_tfidf = tfidf_mdl[corpus]\n",
    "print(len(corpus_tfidf))\n",
    "\n",
    "# view one resume\n",
    "for doc in corpus_tfidf[:1]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "n_features = 1500\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(input='content', ngram_range=(1, 3), max_df=0.85, min_df=0.05, \n",
    "                max_features=n_features, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfidf_vec_prep = tfidf_vec.fit_transform(resumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=5, init='k-means++', max_iter=100, n_init=1, n_jobs=-1)\n",
    "\n",
    "km_mdl = km.fit_predict(tfidf_vec_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6375"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(km_mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine your k range\n",
    "k_range = range(1,20)\n",
    "\n",
    "# fit the kmeans model for each n_clusters = k\n",
    "k_means_var = [KMeans(n_clusters=k).fit(tfidf_vec_prep) for k in k_range]\n",
    "\n",
    "# pull out the cluster centers for each model\n",
    "centroids = [X.cluster_centers_ for X in k_means_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "# calculate the euclidean distance from each point to each cluster center\n",
    "k_euclid = [cdist(tfidf_vec_prep.toarray(), cent, 'euclidean') for cent in centroids]\n",
    "dist = [np.min(ke, axis=1) for ke in k_euclid]\n",
    "\n",
    "# total within-cluster sum of squares\n",
    "wcss = [sum(d**2) for d in dist]\n",
    "\n",
    "# the total sum of squares\n",
    "tss = sum(pdist(tfidf_vec_prep.toarray())**2)/tfidf_vec_prep.shape[1]\n",
    "\n",
    "# the between-cluster sum of squares\n",
    "bss = tss - wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_palette(\"Set2\")\n",
    "colors = sns.color_palette(\"BrBG\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/axes/_axes.py:531: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "# make figure\n",
    "fig = plt.figure(figsize=(20,12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# color\n",
    "colors = sns.color_palette(\"BrBG\", 5)\n",
    "\n",
    "# plots\n",
    "ax.plot(K, avgWithinSS, marker='o', color=colors[-1], alpha=0.5)\n",
    "\n",
    "# labels/titles\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Elbow for K-Means')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Avg. Within-Cluster Sum of Squares')\n",
    "\n",
    "# remove border\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# show grid\n",
    "ax.xaxis.grid(True, alpha=0.2) \n",
    "ax.yaxis.grid(True, alpha=0.2) \n",
    "\n",
    "# plot that biddy\n",
    "plt.savefig('data/{0}.png'.format('KMeans_elbow'), bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.vq import kmeans\n",
    "from scipy.spatial.distance import cdist,pdist\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# perform PCA dimensionality reduction\n",
    "pca = RandomizedPCA(n_components=2).fit(tfidf_vec_prep.toarray())\n",
    "X = pca.transform(tfidf_vec_prep.toarray())\n",
    "\n",
    "##### cluster data into K=1..20 clusters #####\n",
    "K_MAX = 20\n",
    "KK = range(1,K_MAX+1)\n",
    "\n",
    "KM = [kmeans(X,k) for k in KK]\n",
    "centroids = [cent for (cent,var) in KM]\n",
    "D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "cIdx = [np.argmin(D,axis=1) for D in D_k]\n",
    "dist = [np.min(D,axis=1) for D in D_k]\n",
    "\n",
    "tot_withinss = [sum(d**2) for d in dist]  # Total within-cluster sum of squares\n",
    "totss = sum(pdist(X)**2)/X.shape[0]       # The total sum of squares\n",
    "betweenss = totss - tot_withinss          # The between-cluster sum of squares\n",
    "\n",
    "##### plots #####\n",
    "kIdx = 4        # K=10\n",
    "clr = cm.spectral( np.linspace(0,1,10) ).tolist()\n",
    "mrk = 'os^p<dvh8>+x.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/axes/_axes.py:531: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "# make figure\n",
    "fig = plt.figure(figsize=(20,12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# color\n",
    "colors = sns.color_palette(\"BrBG\", 5)\n",
    "\n",
    "# plots\n",
    "ax.plot(KK, betweenss/totss*100, marker='o', color=colors[-1], alpha=0.5)\n",
    "ax.plot(KK[kIdx], betweenss[kIdx]/totss*100, marker='o', markersize=25, color=colors[0], alpha=0.5)\n",
    "\n",
    "# labels/titles\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Elbow for KMeans Clustering')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Percentage of variance explained (%)')\n",
    "\n",
    "ax.set_xlim((-0.1,20.5))\n",
    "ax.set_ylim((-0.5,100))\n",
    "\n",
    "# remove border\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# show grid\n",
    "ax.xaxis.grid(True, alpha=0.2) \n",
    "ax.yaxis.grid(True, alpha=0.2) \n",
    "\n",
    "# plot that biddy\n",
    "plt.savefig('data/{0}.png'.format('KMeans_elbow_var'), bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make figure\n",
    "fig = plt.figure(figsize=(20,12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plots\n",
    "for i in range(kIdx+1):\n",
    "    ind = (cIdx[kIdx]==i)\n",
    "    ax.scatter(X[ind,0],X[ind,1], s=65, c=colors[i], marker=mrk[i], \n",
    "               label='Cluster {0}'.format(i), alpha=1)\n",
    "\n",
    "# labels/titles\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('K={0} Clusters'.format(KK[kIdx]))\n",
    "\n",
    "#ax.set_xlim((-.5,.5))\n",
    "#ax.set_ylim((-.3,.81))\n",
    "\n",
    "# remove border\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# show grid\n",
    "ax.xaxis.grid(True, alpha=0.2) \n",
    "ax.yaxis.grid(True, alpha=0.2) \n",
    "\n",
    "# plot that biddy\n",
    "plt.savefig('data/{0}.png'.format('KMeans_{0}_clusters'.format(KK[kIdx])), bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Indexing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_topics = 100\n",
    "\n",
    "# initialize an LSI transformation\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '0.857*\"|\" + 0.187*\"docker\" + 0.078*\"container\" + 0.075*\"metrics\" + 0.071*\"=\" + 0.063*\">>>\" + 0.054*\"<\" + 0.054*\">\" + 0.054*\"false\" + 0.052*\"<br\"')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the topics are printed to log\n",
    "a = lsi.print_topics(8)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in corpus_lsi[800]: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    pass\n",
    "    #print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi.save('pkl/lsi_mdl.lsi')\n",
    "lsi = models.LsiModel.load('pkl/lsi_mdl.lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_mdl = models.LdaModel(corpus, id2word=dictionary, num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9,\n",
      "  '0.061*brooklyn + 0.015*the + 0.009*apache + 0.007*file + 0.007*license + '\n",
      "  '0.006*= + 0.006*data + 0.005*example + 0.005*this + 0.005*use'),\n",
      " (6,\n",
      "  '0.230*| + 0.065*aurora + 0.028*> + 0.022*<br + 0.017*f + 0.007*mesos + '\n",
      "  '0.007*s + 0.006*orderedlist + 0.005*users + 0.004*scheduler'),\n",
      " (3,\n",
      "  '0.014*git + 0.009*apache + 0.008*task + 0.007*branch + 0.007*new + '\n",
      "  '0.007*code + 0.007*project + 0.007*the + 0.006*create + 0.006*if'),\n",
      " (18,\n",
      "  '0.068*% + 0.009*data + 0.007*the + 0.007*use + 0.007*> + 0.006*using + '\n",
      "  '0.005*site + 0.005*$ + 0.005*html + 0.005*this'),\n",
      " (4,\n",
      "  '0.023*> + 0.017*br + 0.011*% + 0.009*= + 0.008*dec + 0.008*web + 0.007*py + '\n",
      "  '0.007*mysql + 0.007*bind + 0.007*nginx'),\n",
      " (14,\n",
      "  '0.016*– + 0.012*false + 0.011*default + 0.010*the + 0.010*| + 0.009*server '\n",
      "  '+ 0.008*type + 0.008*docker + 0.008*example + 0.007*='),\n",
      " (15,\n",
      "  '0.087*< + 0.072*td> + 0.030*tr> + 0.026*<tr> + 0.017*<td> + 0.013*= + '\n",
      "  '0.010*> + 0.009*the + 0.007*<td><a + 0.007*class='),\n",
      " (19,\n",
      "  '0.055*= + 0.022*>>> + 0.015*return + 0.014*name + 0.011*value + '\n",
      "  '0.010*datetime + 0.010*python + 0.009*int + 0.009*self + 0.008*type'),\n",
      " (17,\n",
      "  '0.018*> + 0.011*data + 0.010*the + 0.009*< + 0.007*file + 0.007*use + '\n",
      "  '0.006*command + 0.006*time + 0.006*$v + 0.006*='),\n",
      " (0,\n",
      "  '0.034*e + 0.025*b + 0.017*c + 0.010*example + 0.010*f + 0.008*name + '\n",
      "  '0.008*< + 0.008*fa + 0.007*bson + 0.006*=')]\n"
     ]
    }
   ],
   "source": [
    "lda_mdl.top_topics\n",
    "pprint(lda_mdl.print_topics(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(17049 documents, 42606 features, 3131686 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2.374975010869965), (1, 0.51728887522253952), (2, -0.058935199530753268), (3, 0.3578493537974749), (4, 1.560417648600577), (5, -1.9931029846659232), (6, 0.58697139609914861), (7, 1.437193124041608), (8, -0.38633595032575146), (9, -2.3068352804125016), (10, 0.77482570234627612), (11, -0.66082521176920128), (12, -2.0221618401059822), (13, 1.3229424544863675), (14, -0.29408524037515837), (15, -1.0569710323996966), (16, 1.110889840043604), (17, 1.3434022602282594), (18, -0.095802335904933394), (19, -0.80089048085959047), (20, -0.64832039201675884), (21, 1.35059095621303), (22, 0.36313071163680766), (23, 0.23008512654094881), (24, -1.4704302056681957), (25, -0.51110545886820391), (26, 1.5065962351771218), (27, -0.85864630999976976), (28, -0.27005311330166226), (29, 1.3357001963834654), (30, 0.11920370036201439), (31, 0.20935482520268536), (32, 0.58140672694418549), (33, 0.86476990150558442), (34, 0.21906262257842274), (35, 1.2623527033747142), (36, 0.47122700487966684), (37, 0.14754992485952445), (38, -0.029780850257687785), (39, 0.41251322337680407), (40, 0.70805306705532289), (41, -0.17539941089750521), (42, 0.099208258486715051), (43, 0.52714882842769772), (44, -0.55353450448882024), (45, -0.48520621106869544), (46, 0.42932852481533534), (47, -1.0848551994364626), (48, -0.2278193012580656), (49, -0.86398865304435535), (50, 0.26069692321941718), (51, -0.17035678155826239), (52, 0.17694402303837284), (53, 0.38019775252075771), (54, 0.52907741665760166), (55, -0.56801027798438197), (56, -0.24289558061900623), (57, -0.53166839270636368), (58, -0.75397485089313621), (59, 0.43914810153445505), (60, -0.11539391176838343), (61, 0.28098629645010242), (62, -0.22417217147281987), (63, 0.04359834386371364), (64, 0.40124504321511811), (65, 0.74406715148428892), (66, 0.083025633287427653), (67, -0.56067477401379284), (68, 0.22243465345106417), (69, -0.39550436325219973), (70, -0.54147531866201193), (71, -0.55283044224248479), (72, -1.619913100721621), (73, -0.093405314999276637), (74, 0.30444920349708604), (75, -0.53813981022164803), (76, -0.59617088497008486), (77, -0.51219246727570034), (78, -0.13706180463557627), (79, -0.16008030773188894), (80, -0.95552532874370033), (81, -1.0713657346866474), (82, -0.39524155791968052), (83, 0.10409521414708364), (84, -0.52691807273338676), (85, 0.28081975514224211), (86, -0.93232856873163084), (87, -0.18390081515478202), (88, -0.46222984135156353), (89, 0.1668585124747386), (90, 0.87547572965713072), (91, 0.037415066391670221), (92, -0.48772947456671473), (93, -0.41313026558553678), (94, 0.85224037332425129), (95, -0.25790488005477619), (96, -0.023718854903863967), (97, 0.32059833574508628), (98, -0.24697257256407545), (99, 0.41432508817899638)]\n"
     ]
    }
   ],
   "source": [
    "doc = df.iloc[0]['resume_nouns']\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index.save('pkl/resume_stopped.index')\n",
    "index = similarities.MatrixSimilarity.load('pkl/resume_stopped.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "\n",
    "# (document_number, document_similarity)\n",
    "sim_lst = list(enumerate(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sim_lst.sort(key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(537, 0.9609338),\n",
       " (468, 0.95680636),\n",
       " (39, 0.95674884),\n",
       " (189, 0.95360476),\n",
       " (737, 0.94994313)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing resumes within resumes\n",
    "sim_lst[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'engineer structural engineer december nonstructural equipment hospitals accordance asce cbc local codes extensive knowledge experience engineer programs enercalc etabs hilti profis remodel buildings beams columns foundations physical work remodel ensure work civil engineer student worker department public works september engineer meet publics needs transportation infrastructure project engineer project manages geographic presentation data gis system engineer report documents fund multimillion projects microsoft word access multiple projects coordination disaster reimbursement civil engineer'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
