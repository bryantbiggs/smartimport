{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8495\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is this Python project Describe features ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contributing Your contributions are always wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awesome Python Awesome A curated list of aweso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contribution Guidelines Before opening any iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contributor Code of Conduct As contributors an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file_contents\n",
       "0  What is this Python project Describe features ...\n",
       "1  Contributing Your contributions are always wel...\n",
       "2  Awesome Python Awesome A curated list of aweso...\n",
       "3  Contribution Guidelines Before opening any iss...\n",
       "4  Contributor Code of Conduct As contributors an..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_json('data/md_contents.json')\n",
    "df_2 = pd.read_json('data/rst_contents.json')\n",
    "print(len(df_1))\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19374\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Requests is written and maintained by Kenneth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>changelog Release History Bugfixes Fixed a bug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you are planning to submit a pull request t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>api Developer Interface module requests This p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faq Frequently Asked Questions This part of th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file_contents\n",
       "0  Requests is written and maintained by Kenneth ...\n",
       "1  changelog Release History Bugfixes Fixed a bug...\n",
       "2  If you are planning to submit a pull request t...\n",
       "3  api Developer Interface module requests This p...\n",
       "4  faq Frequently Asked Questions This part of th..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_2))\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27869\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is this Python project Describe features ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contributing Your contributions are always wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awesome Python Awesome A curated list of aweso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contribution Guidelines Before opening any iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contributor Code of Conduct As contributors an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file_contents\n",
       "0  What is this Python project Describe features ...\n",
       "1  Contributing Your contributions are always wel...\n",
       "2  Awesome Python Awesome A curated list of aweso...\n",
       "3  Contribution Guidelines Before opening any iss...\n",
       "4  Contributor Code of Conduct As contributors an..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [df_1, df_2]\n",
    "\n",
    "df = pd.concat(frames)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract just nouns/noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def only_nouns(sentence):\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    sent = pos_tag(sentence)\n",
    "    return ' '.join([s[0].lower() for s in sent if s[1] in ['NN', 'NNP']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nouns = lambda x: only_nouns(x)\n",
    "\n",
    "df['file_nouns'] = df['file_contents']\n",
    "df['file_nouns'] = df['file_nouns'].apply(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_contents</th>\n",
       "      <th>file_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is this Python project Describe features ...</td>\n",
       "      <td>python project describe difference python proj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contributing Your contributions are always wel...</td>\n",
       "      <td>guidelines add link pull request add link proj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awesome Python Awesome A curated list of aweso...</td>\n",
       "      <td>awesome python awesome a list python framework...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contribution Guidelines Before opening any iss...</td>\n",
       "      <td>contribution guidelines read contributor s gui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contributor Code of Conduct As contributors an...</td>\n",
       "      <td>contributor code conduct project interest comm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file_contents  \\\n",
       "0  What is this Python project Describe features ...   \n",
       "1  Contributing Your contributions are always wel...   \n",
       "2  Awesome Python Awesome A curated list of aweso...   \n",
       "3  Contribution Guidelines Before opening any iss...   \n",
       "4  Contributor Code of Conduct As contributors an...   \n",
       "\n",
       "                                          file_nouns  \n",
       "0  python project describe difference python proj...  \n",
       "1  guidelines add link pull request add link proj...  \n",
       "2  awesome python awesome a list python framework...  \n",
       "3  contribution guidelines read contributor s gui...  \n",
       "4  contributor code conduct project interest comm...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Series to List of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'guidelines add link pull request add link project name a description period keep add section section description add section title table contents search yours don t mention python description check spelling grammar remove whitespace send pull request reason library'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = df['file_nouns'].tolist()\n",
    "contents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    # filter words shorter than 2 chars and words containing chars other than letters\n",
    "    for token in tokens:\n",
    "        if len(token) > 2:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                if token not in stopwords:\n",
    "                    filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_tokenized = []\n",
    "for doc in contents:\n",
    "    doc_tokens = tokenize(doc)\n",
    "    vocab_tokenized.extend(doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4740829\n",
      "118803\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_tokenized))\n",
    "print(len(set(vocab_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4740829 words in tokenized corpus\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': vocab_tokenized}, index = vocab_tokenized)\n",
    "print('{0} words in tokenized corpus'.format(len(vocab_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>python</th>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>describe</th>\n",
       "      <td>describe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difference</th>\n",
       "      <td>difference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>python</th>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 words\n",
       "python          python\n",
       "project        project\n",
       "describe      describe\n",
       "difference  difference\n",
       "python          python"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118803"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc = Counter(vocab_tokenized)\n",
    "\n",
    "y = OrderedDict(wc.most_common()[::-1])\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 1.48 s, total: 1min 31s\n",
      "Wall time: 1min 31s\n",
      "(27869, 1132)\n"
     ]
    }
   ],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.98, max_features=1500, min_df=0.01, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(contents)\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.8 s, sys: 281 ms, total: 39 s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=5)\n",
    "%time km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "joblib.dump(km,  'pkl/doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('pkl/doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = { 'contents': contents, 'cluster': clusters }\n",
    "frame = pd.DataFrame(data, index = [clusters] , columns = ['contents', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1083,  346,  469, ...,  248,  678,  739],\n",
       "       [  66,  497,  657, ...,  631,  630,    0],\n",
       "       [ 786,  154,  385, ..., 1084, 1086,   67],\n",
       "       [ 377, 1085, 1083, ...,  917,  464,  371],\n",
       "       [ 868,   67,  869, ...,  615,  295,  976]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "order_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0: b'vim' , b'event' , b'host' , b'dynamicdata' , b'attributes' , b'vmodl' , b'device' , b'api' ,\n",
      "\n",
      "Cluster 1: b'automodule' , b'inheritance' , b'module' , b'module' , b'mod' , b'package' , b'import' , b'autoclass' ,\n",
      "\n",
      "Cluster 2: b'python' , b'code' , b'file' , b'class' , b'docker' , b'example' , b'image' , b'import' ,\n",
      "\n",
      "Cluster 3: b'fault' , b'vim' , b'vim' , b'str' , b'str' , b'host' , b'attributes' , b'machine' ,\n",
      "\n",
      "Cluster 4: b'salt' , b'automodule' , b'salt' , b'automodule' , b'module' , b'module' , b'cloud' , b'master' ,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Cluster {0}:\".format(i), end='')\n",
    "    for ind in order_centroids[i, :8]: #replace 8 with n words per cluster\n",
    "        print(' %s ' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Strings to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the documents, remove stop words and words that only appear once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove common words and tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27869"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [[word.lower() for word in content.split()if word.lower() not in stoplist] for content in contents]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# remove words that occur less than n times\n",
    "texts = [[token for token in text if frequency[token] > 3] for text in texts]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Token Count Dictionary to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(41107 unique tokens: ['giza', 'schofield', 'sshfp', 'composewith', 'kwlist']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# store the dictionary, for future reference\n",
    "dictionary.save('data/text_token.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Tokenized Resumes to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('data/text_token.mm', corpus)  # store to disk, for later use\n",
    "for c in corpus[:1]:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dictionary LOADED as 'dictionary'\n"
     ]
    }
   ],
   "source": [
    "# load tokenized dictionary\n",
    "if (os.path.exists('data/text_token.dict')):\n",
    "    dictionary = corpora.Dictionary.load('data/text_token.dict')\n",
    "    print('Tokenized dictionary LOADED as \\'dictionary\\'')\n",
    "else:\n",
    "    print('Tokenized dictionary NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix LOADED as 'corpus'\n"
     ]
    }
   ],
   "source": [
    "# load sparse vector matrix\n",
    "if (os.path.exists('data/text_token.mm')):\n",
    "    corpus = corpora.MmCorpus('data/text_token.mm')\n",
    "    print('Sparse matrix LOADED as \\'corpus\\'')\n",
    "else:\n",
    "    print('Sparse matrix NOT FOUND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step 1 -- initialize a model\n",
    "tfidf_mdl = models.TfidfModel(corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `model[corpus]` only creates a wrapper around the old corpus document stream – actual conversions are done on-the-fly, during document iteration. We cannot convert the entire corpus at the time of calling corpus_transformed = model[corpus], because that would mean storing the result in main memory, and that contradicts gensim’s objective of memory-indepedence. If you will be iterating over the transformed corpus_transformed multiple times, and the transformation is costly, serialize the resulting corpus to disk first and continue using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27869\n",
      "[(0, 0.19075830486731032), (1, 0.4682655254274302), (2, 0.3599113037331229), (3, 0.3871385595612281), (4, 0.28721377848257035), (5, 0.1969951695186765), (6, 0.3918408487564352), (7, 0.35733996735093254), (8, 0.24979251706536024)]\n"
     ]
    }
   ],
   "source": [
    "# step 2 -- use the model to transform vectors\n",
    "corpus_tfidf = tfidf_mdl[corpus]\n",
    "print(len(corpus_tfidf))\n",
    "\n",
    "# view one resume\n",
    "for doc in corpus_tfidf[:1]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "n_features = 1500\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(input='content', ngram_range=(1, 3), max_df=0.85, min_df=0.05, \n",
    "                max_features=n_features, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfidf_vec_prep = tfidf_vec.fit_transform(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=5, init='k-means++', max_iter=100, n_init=1, n_jobs=-1)\n",
    "\n",
    "km_mdl = km.fit_predict(tfidf_vec_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27869"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(km_mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine your k range\n",
    "k_range = range(1,20)\n",
    "\n",
    "# fit the kmeans model for each n_clusters = k\n",
    "k_means_var = [KMeans(n_clusters=k).fit(tfidf_vec_prep) for k in k_range]\n",
    "\n",
    "# pull out the cluster centers for each model\n",
    "centroids = [X.cluster_centers_ for X in k_means_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "# calculate the euclidean distance from each point to each cluster center\n",
    "k_euclid = [cdist(tfidf_vec_prep.toarray(), cent, 'euclidean') for cent in centroids]\n",
    "dist = [np.min(ke, axis=1) for ke in k_euclid]\n",
    "\n",
    "# total within-cluster sum of squares\n",
    "wcss = [sum(d**2) for d in dist]\n",
    "\n",
    "# the total sum of squares\n",
    "tss = sum(pdist(tfidf_vec_prep.toarray())**2)/tfidf_vec_prep.shape[1]\n",
    "\n",
    "# the between-cluster sum of squares\n",
    "bss = tss - wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "colors = ['#fffc31', '#ff9505', '#ff3c38', '#04e762', '#662c91']\n",
    "edges = ['#ffb901', '#cc5803', '#eb3968', '#00501e', '#ad66ff']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# make figure\n",
    "fig = plt.figure(figsize=(20,12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# color\n",
    "colors = sns.color_palette(\"BrBG\", 5)\n",
    "\n",
    "# plots\n",
    "ax.plot(K, avgWithinSS, marker='o', color=colors[-1], alpha=0.5)\n",
    "\n",
    "# labels/titles\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Elbow for K-Means')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Avg. Within-Cluster Sum of Squares')\n",
    "\n",
    "# remove border\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# show grid\n",
    "ax.xaxis.grid(True, alpha=0.2) \n",
    "ax.yaxis.grid(True, alpha=0.2) \n",
    "\n",
    "# plot that biddy\n",
    "plt.savefig('data/{0}.png'.format('KMeans_elbow'), bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.vq import kmeans\n",
    "from scipy.spatial.distance import cdist,pdist\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# perform PCA dimensionality reduction\n",
    "pca = RandomizedPCA(n_components=2).fit(tfidf_vec_prep.toarray())\n",
    "X = pca.transform(tfidf_vec_prep.toarray())\n",
    "\n",
    "##### cluster data into K=1..20 clusters #####\n",
    "K_MAX = 20\n",
    "KK = range(1,K_MAX+1)\n",
    "\n",
    "KM = [kmeans(X,k) for k in KK]\n",
    "centroids = [cent for (cent,var) in KM]\n",
    "D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "cIdx = [np.argmin(D,axis=1) for D in D_k]\n",
    "dist = [np.min(D,axis=1) for D in D_k]\n",
    "\n",
    "tot_withinss = [sum(d**2) for d in dist]  # Total within-cluster sum of squares\n",
    "totss = sum(pdist(X)**2)/X.shape[0]       # The total sum of squares\n",
    "betweenss = totss - tot_withinss          # The between-cluster sum of squares\n",
    "\n",
    "##### plots #####\n",
    "kIdx = 4        # K=10\n",
    "clr = cm.spectral( np.linspace(0,1,10) ).tolist()\n",
    "mrk = 'os^p<dvh8>+x.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/axes/_axes.py:531: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "# make figure\n",
    "fig = plt.figure(figsize=(20,12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plots\n",
    "ax.plot(KK, betweenss/totss*100, marker='o', color=colors[-1], alpha=0.5)\n",
    "ax.plot(KK[kIdx], betweenss[kIdx]/totss*100, marker='o', markersize=25, color=colors[0], alpha=0.5)\n",
    "\n",
    "# labels/titles\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Elbow for KMeans Clustering')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Percentage of variance explained (%)')\n",
    "\n",
    "ax.set_xlim((-0.1,20.5))\n",
    "ax.set_ylim((-0.5,100))\n",
    "\n",
    "# remove border\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# show grid\n",
    "ax.xaxis.grid(True, alpha=0.2) \n",
    "ax.yaxis.grid(True, alpha=0.2) \n",
    "\n",
    "# plot that biddy\n",
    "plt.savefig('data/{0}.png'.format('KMeans_elbow_var'), bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make figure\n",
    "fig = plt.figure(figsize=(20,12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plots\n",
    "for i in range(kIdx+1):\n",
    "    ind = (cIdx[kIdx]==i)\n",
    "    ax.scatter(X[ind,0],X[ind,1], s=65, c=colors[i], marker=mrk[i], \n",
    "               label='Cluster {0}'.format(i), alpha=1)\n",
    "\n",
    "# labels/titles\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('K={0} Clusters'.format(KK[kIdx]))\n",
    "\n",
    "#ax.set_xlim((-.5,.5))\n",
    "#ax.set_ylim((-.3,.81))\n",
    "\n",
    "# remove border\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# show grid\n",
    "ax.xaxis.grid(True, alpha=0.2) \n",
    "ax.yaxis.grid(True, alpha=0.2) \n",
    "\n",
    "# plot that biddy\n",
    "plt.savefig('data/{0}.png'.format('KMeans_{0}_clusters'.format(KK[kIdx])), bbox_inches='tight')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "colors = ['#fffc31', '#ff9505', '#ff3c38', '#04e762', '#662c91']\n",
    "edges = ['#ffb901', '#cc5803', '#CF000F', '#00501e', '#ad66ff']\n",
    "labels = ['Web', 'Programs', 'Dev Ops', 'Modules', 'Device']\n",
    "#alphas = [0.0, 1, 0.0, 0.0, 0.0]\n",
    "alphas = [1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perform PCA dimensionality reduction\n",
    "pca = RandomizedPCA(n_components=3).fit(tfidf_vec_prep.toarray())\n",
    "X = pca.transform(tfidf_vec_prep.toarray())\n",
    "\n",
    "##### cluster data into K=1..20 clusters #####\n",
    "K_MAX = 20\n",
    "KK = range(1,K_MAX+1)\n",
    "\n",
    "KM = [kmeans(X,k) for k in KK]\n",
    "centroids = [cent for (cent,var) in KM]\n",
    "D_k = [cdist(X, cent, 'euclidean') for cent in centroids]\n",
    "cIdx = [np.argmin(D,axis=1) for D in D_k]\n",
    "dist = [np.min(D,axis=1) for D in D_k]\n",
    "\n",
    "tot_withinss = [sum(d**2) for d in dist]  # Total within-cluster sum of squares\n",
    "totss = sum(pdist(X)**2)/X.shape[0]       # The total sum of squares\n",
    "betweenss = totss - tot_withinss          # The between-cluster sum of squares\n",
    "\n",
    "##### plots #####\n",
    "kIdx = 4        # K=10\n",
    "clr = cm.spectral( np.linspace(0,1,10) ).tolist()\n",
    "mrk = 'os^p<dvh8>+x.'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.font_manager\n",
    "matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "\n",
    "prop = fm.FontProperties(fname='/Users/bryant/Library/Fonts/BebasNeue Regular.otf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(18,16))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Get rid of the panes                          \n",
    "ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0)) \n",
    "ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0)) \n",
    "ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0)) \n",
    "\n",
    "# Get rid of the spines                         \n",
    "ax.w_xaxis.line.set_color((1.0, 1.0, 1.0, 0.0)) \n",
    "ax.w_yaxis.line.set_color((1.0, 1.0, 1.0, 0.0)) \n",
    "ax.w_zaxis.line.set_color((1.0, 1.0, 1.0, 0.0))\n",
    "\n",
    "#ax.set_axis_off()\n",
    "\n",
    "# plots\n",
    "for i in range(kIdx+1):\n",
    "    ind = (cIdx[kIdx]==i)\n",
    "    ax.scatter(X[ind,0],X[ind,1], X[ind,2], s=105, c=colors[i], alpha=alphas[i], marker=mrk[i], \n",
    "               label=labels[i], edgecolor=edges[i])\n",
    "\n",
    "# labels/titles\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "ax.set_xlim((-0.4,0.5))\n",
    "ax.set_ylim((0.0,1.2))\n",
    "ax.set_zlim((-0.6,0.35))\n",
    "\n",
    "\n",
    "plt.savefig('data/{0}.png'.format('KMeans_{0}_3D'.format(KK[kIdx])), bbox_inches='tight', dpi=150,)\n",
    "plt.close(fig)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Indexing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_topics = 100\n",
    "\n",
    "# initialize an LSI transformation\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the topics are printed to log\n",
    "a = lsi.print_topics(8)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in corpus_lsi[800]: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    pass\n",
    "    #print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi.save('pkl/lsi_mdl.lsi')\n",
    "lsi = models.LsiModel.load('pkl/lsi_mdl.lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_mdl = models.LdaModel(corpus, id2word=dictionary, num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_mdl.top_topics\n",
    "pprint(lda_mdl.print_topics(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = df.iloc[0]['resume_nouns']\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index.save('pkl/resume_stopped.index')\n",
    "index = similarities.MatrixSimilarity.load('pkl/resume_stopped.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "\n",
    "# (document_number, document_similarity)\n",
    "sim_lst = list(enumerate(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sim_lst.sort(key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# comparing resumes within resumes\n",
    "sim_lst[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' '.join(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
